# Reinforcement Learning never worked, and 'deep' only helped a bit
tags:
- [[reinforcement-learning]]

refrence:
```
@misc{rlblogpost,
    title={Reinforcement Learning never worked, and 'deep' only helped a bit},
    author={Sahni, Himanshu},
    howpublished={\url{https://himanshusahni.github.io/2018/02/23/reinforcement-learning-never-worked.html}},
    year={2018}
}
```

#### The two main problems of RL
1. the balance of exploration vs. exploitation
2. long term credit assignment

#### the balance of exploration vs. exploitation
> The famous Bellman update only guarantees convergence to the optimal value function if every state is visited an infinite number of times and every action is tried an infinite number of times in it. So right off the bat, we need an infinite samples to learn, and we need them everywhere!
> Exploration in uncertain dynamics also explains why RL seems to be more sensitive to hyper-parameters and random seeds than supervised learning. There are no fixed datasets your networks are training on. The training data is directly dependent on the network output, whatever exploration mechanism you use, and environment randomness. Therefore, with the same algorithm on the same environment in different runs, you may see dramatically different training sets, leading to dramatically different performance (take a look at [4]). Again, the core problem is that of controlling exploration to see similar distributions of states, something that the most general algorithms make no assumptions over.
> For continuous action spaces, the most popular methods are on-policy. On-policy methods can only use samples consistent with the current policy that is being executed. This also means that as soon as you update your current policy, everything you have experienced in the past becomes immediately unusable. Most algorithms you hear of in the weird domain with orange humans and animals that look like a bunch of piping (Mujoco), are on-policy.
> Off-policy methods on the other hand, can learn the optimal policy by observing any other policy being executed. This is obviously much nicer but unfortunately we are just not that good at it yet.

#### Long term credit assignment
> The problem actually occurs because the scale at which we can provide rewards for meaningful tasks is much larger than the scale current day algorithms can handle. There are two solutions to this. One is to reduce the scale at which rewards are provided, i.e. provide shaping rewards more frequently. As usual, though, if you give an optimization algorithm a weakness, it will exploit it all the way to optimality.
> Are there better ways of specifying goals? In imitation learning, you can slyly sidestep the whole RL problem by asking for labels directly from the target distribution, i.e. the optimal policy. There are other ways of learning without direct rewards [5], or providing goals to agents as images [6]. (Stay tuned for an ICML workshop on Goal Specification in RL!)

> Another promising way to deal with long horizons (hugely delayed rewards) is hierarchical reinforcement learning. Hierarchical RL attempts to decompose a long horizon problem into a series of goals and subgoals. By decomposing the problem, we are effectively dilating the time scale at which decisions are being made. The really exciting stuff is if the policies being learned for subgoals can also be applied to achieving other goals.

[//begin]: # "Autogenerated link references for markdown compatibility"
[reinforcement-learning]: ..\reinforcement-learning "Reinforcement Learning"
[//end]: # "Autogenerated link references"