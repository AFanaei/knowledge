# All blog posts of https://www.alexirpan.com/
#### [Dota 2 1v1](https://www.alexirpan.com/2017/08/12/openai-dota-2.html) and [5v5](https://www.alexirpan.com/2018/06/27/dota-2-five.html)
It could even be using the system from the [Learning from Human Preferences](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) paper - I could see it being useful for given partial signal to behaviors that are hard to define.
Speaking of saying “screw it” and doing the thing that will clearly scale, it’s interesting that plain PPO is just good enough so far. I’m most surprised by the time horizon problem. The partial observability hurts, but empirically it was doable for the Dota 1v1 bot. The high dimensional action / observation space didn’t feel like obstacles to me - they looked annoying but didn’t look impassable. But the long time horizons problem felt hard enough that I expected it to require something besides just PPO.

This seems to have parallels to the Retro Contest results, where the winning entries were just tuned versions of PPO and Rainbow DQN. In the past, I’ve been skeptical of the “hardware hypothesis”, where the only thing stopping AI progress is faster computers. At the time, I said I thought the split in AI capabilities was about 50-50 between hardware and software. I’m starting to lean towards the hardware side, updating towards something like 60-40 for hardware vs software. There are an increasing number of results where baseline algorithms just work if you try them at the right scale, enough that I can’t ignore them.

Learning algorithm: Probably [PPO](https://openai.com/blog/openai-baselines-ppo/). It works well, and it supports continuous control, which may be a better fit for mouse movement.

#### [The Friendship Paradox And You](https://www.alexirpan.com/2017/09/13/friendship-paradox.html)
**not related to RL but a fun post to read.**

Well, for one, it’s great for addressing imposter syndrome issues. For example, sometimes I feel like I should be writing more. When I poke at the feeling, it often turns into this.

1. I should write more.
2. Why do I think that? It’s partly because I read cool things from people that write more than I do.
3. But by friendship paradox, it’s expected that those writers are writing more than me.
4. So hey, maybe I shouldn’t feel too bad.

More importantly, the friendship paradox touches on another, more important idea: the things you see don’t have to reflect reality. If you base your assumptions of popularity on the popularity of your friends, on average you’ll come up short. If you base your assumptions of productivity by things you read online, it’s easier to see evidence of productivity from people who are very productive. If you base your views of somebody by things you hear them say, it’s warped by the chances you would have heard their views in the first place. And so on down.

#### [A Lagrange Multipliers Refresher (adverserial prof)](https://www.alexirpan.com/2019/07/27/lagrange-multipliers.html)

Let’s consider a variant of the problem. We want to minimize f(x) subject to g(x)≥0.

​min ​f(x)
​subject to ​g(x)≥0

Let’s define the following min-max optimization problem.

​min_{x}(​max_{λ≥0}(​​f(x)−λg(x)))
​
I claim the solution x of this optimization problem occurs at the smallest f(x) that satisfies the constraint g(x)≥0. Why?

As written, we first choose an x, then choose a λ that maximizes the objective, and we want to choose an x that minimizes how much an adversarial λ can hurt us. Suppose we are violating the constraint g(x)≥0. Then we have g(x)<0. At such an x, −g(x)>0, so we can pick λ=∞ to drive the objective value to ∞.

If we are not violating the constraint, then −g(x) is 0 or negative, and since λ is constrained to λ≥0, the optimal λ is λ=0, giving objective value f(x).

In other words, the solution of the unconstrained problem is the same as the solution to the original constrained problem.

Bringing it back to the equality case, let’s say we have a constraint g(x)=c. How do we reduce this to the previous cases? This equality is the same as the pair of constraints g(x)≥c,g(x)≤c, which are only both satisfied at g(x)=c. This time, I’ll write the Lagrange multipliers as λ_1 and λ_2

​min ​f(x)
​subject to ​g(x)≥c
​subject to ​g(x)≤c
​​

min_{x}(max_{λ_1≥0, λ_2≥0} f(x) - λ_1(g(x)−c) − λ_2(c−g(x))

Like before, if we ever have g(x)≠c, then we can choose λ_1, λ_2 such that the objective value shoots up to ∞. But, since g(x)−c and c−g(x) are just negations of each other, we can simplify this further.

min_{x}(max_{λ_1≥0, λ_2≥0} f(x) - (λ_1-λ_2)(g(x)−c)

It’s possible to make λ_1-λ_2 equal any real number while still satisfying λ_1≥0, λ_2≥0, so let’s just replace λ_1-λ_2 with λ, and say λ can be any real number instead of only nonnegative ones.

min_{x}(max_{λ} f(x) - λ(g(x)−c)

Now, we’re back to the equality case we started at. The solution to this must lie at a saddle point where the gradient with respect to x and λ is both zero.

#### [OpenAI's Rubik's Cube Result](https://www.alexirpan.com/2019/10/29/openai-rubiks.html)
A lot of it isn’t new. Everything about this paper looks like a [moonshot achieved through roofshots](https://www.alexirpan.com/2019/10/29/openai-rubiks.html) project, where there’s a clear line of steady, compounding improvements from prior work.

The model is trained with distributed PPO using OpenAI’s Rapid framework, which was used for both OpenAI Five and the Learning Dexterity paper. The model architecture is heavily inspired by the DotA 2 architecture - each input feature is embedded into a 512-dimensional embedding space, and these embeddings are summed and passed through a large LSTM.

Like the Learning Dexterity work, instead of learning a policy directly on pixels through RL, they instead predict the pose of the Rubik’s Cube from three camera viewpoints, then feed those predicted poses to the RL agent.

Again, like the Learning Dexterity work, they use the same asymmetric actor-critic trick, where the critic gets all ground truth state information, and the policy only gets the features visible from real-world data, which is fine for zero-shot transfer because you only need the policy at inference time.

It’s the same ideas, likely even the same codebase, executed in a different context.

