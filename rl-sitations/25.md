# [Batch Norm](https://www.alexirpan.com/public/perils-batch-norm/batch_norm_appendix.html)
tags:
- [[deep-learning]]

refrence:
```
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
```
As the parameters change (in a NN during backpropagation), the distribution of each layer’s activations changes too. This makes the next layer’s job harder, because the distribution it’s learning from keeps changing underneath it. Each layer must continually adapt to the distribution of the layer before it, and it seems like this should slow down learning. The literature calls this covariate shift. Generally, it’s been studied at the data level, where the test set distribution differs from the training set distribution.

[batch normalization algorithm](https://www.alexirpan.com/public/perils-batch-norm/batch-norm-alg.png) first μ and σ^2 are calculated for each minibatch and output is modified using this vars to have mean 0 variance equal to one. Finally, in practice we don’t want to limit the output distribution to always be mean 0 and variance 1, because this constrains the network too much for learning. We define y=γBN(x)+β to be the actual output, and let β and γ be learnable parameters.

This kind of breaks the internal covariate shift argument. If the network can still learn any distribution, how have we helped fix the problem? consider one unit in a NN, After applying batch norm, this unit has mean β, variance γ. The statistics of the distribution are concentrated entirely into those two variables. If batch norm wasn’t used, the mean and variance would be implicitly defined by the previous layer activations. When mean and variance are directly optimizable, it’s easier for gradient descent to discover the best distribution for each layer’s activations.

[an example](https://www.alexirpan.com/public/perils-batch-norm/batch-norm-distribution.png) of learning with and without BN.
​​### [Batch Norm issues](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)
note that Whenever individual minibatches aren’t representative of your entire data distribution, you may run into problems. That means forgetting to randomize your input is especially bad with batch norm. It also plays a big role in GANs. The discriminator is usually trained on a mix of fake data and real data. If your discriminator uses batch norm, it’s incorrect to alternate between batches of all fake or all real data. Each minibatch needs to be a 50-50 mix of both.

Batch norm changes models in two fundamental ways.

- At training time, the output for a single input x_i depends on the other x_j in the minibatch.
- At testing time, the model runs a different computation path, because now it normalizes with the moving average instead of the minibatch average.

There’s a famous letter in computer science: Dijkstra’s Go To Statement Considered Harmful. In it, Dijkstra argues that the goto statement should be avoided, because it makes code harder to read, and any program that uses goto can be rewritten to avoid it. I really, really wanted to title this post “Batch Norm Considered Harmful”, but I couldn’t justify it. Batch norm works too well. Yes, it has issues, but when you do everything right, models train a lot faster. No contest. There’s a reason the batch norm paper has over 1400 citations, as of this post.