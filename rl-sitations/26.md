# [Hyperparameter Optimization: A Spectral Approach](https://www.alexirpan.com/2017/06/27/hyperparam-spectral.html)
tags:
- [[deep-learning]]
- [[Hyperparameter]]

refrence:
```
@article{hazan2017hyperparameter,
  title={Hyperparameter optimization: A spectral approach},
  author={Hazan, Elad and Klivans, Adam and Yuan, Yang},
  journal={arXiv preprint arXiv:1706.00764},
  year={2017}
}
```
Hyperparam optimization is a weird problem because everybody has to do it and nobody really likes it. The common options are either grid search, random search, or line search if your parameter is continuous. A mix of those does a pretty good job for most ML algorithms, even deep neural nets, which are a bit notorious for having many hyperparams.

Let’s say there are n hyperparams. Then given all these assumptions, we want to learn a function f:{-1,+1}^n → R, where the input is the hyperparam setting, and the output is the performance of the trained model.

#### The Parity Function Basis

first note that the Boolean functions f:{-1,+1}^n → R form an inner product space. Define the inner product as

⟨f,g⟩=E_x[f(x)g(x)]

it has:
- Symmetry
- Linearity
- Positive definiteness

An orthonormal basis for this inner product space is defined:

Let S⊂[n] be a subset of indicies 1 to n. Define f_S(x) as:

f_S(x) = ​∏_{​i∈S} x_i
​​ 
The functions {f_​S} (known as the parity functions) form an orthonormal basis.
- For every f_i ⟨f_i, f_i⟩=1.
- For every f_i, f_j where i≠j, ⟨f_i, f_j⟩=0.

#### Low Degree Approximation

Let P(n) be the set of all subsets of 1 to n. Then for any f,

f(x) = ​∑_{​S∈P(n)}​​ ⟨f,f_S⟩f_​S(x)

first we should select S∈P(n):∣S∣≤d and then find coefficients using LS to ensure sparcity L1 regularization is added: (T number of evaluation points)

​min λ∗∥α∥_{1} + ​∑_{i=1}^{T} (f(x_i) - ​∑_{​∣S∣≤d} α_S f_S(x_i))^2

#### Harmonica: The Full Algorithm
since g is not depended on all x (some are proned due to ∣S∣≤d and only s big coeffs are selected during Low Degree Approximation) we should proceed iteratively. [full algorithm](https://www.alexirpan.com/public/hyperparam-spectral/harmonica.png)

this method is good because it can show importance of diffrent hyperparameters. [example](https://www.alexirpan.com/public/hyperparam-spectral/results4.png)

#### advantages
In each stage of Harmonica, each model can be evaluated in parallel. By contrast, Bayesian optimization techniques are more difficult to parallelize because the derivation often assumes a single-threaded evaluation.

Enforcing hyperparam sparsity leads to interpretability - weights of different terms can be used to interpret which hyperparams are most important and least important. The approach successfully learned to ignore all dummy hyperparams.

#### Disadvantages
A coworker mentioned that Harmonica assumes a fixed computation budget for each stage, which doesn’t make it as good in an anytime setting (which is a better fit for Bayesian optimization.)

My intuition says that this approach works best when you have a large number of hyperparameters and want to initially narrow down the space. Each stage requires hundreds of models, and in smaller hyperparam spaces, evaluating 100 models with random search is going to enough tuning already. That being said, perhaps you can get away with fewer if you’re working in a smaller hyperparam space.

In the first two stages, Harmonica is run on a smaller 8 layer network, and the full 56 layer Resnet is only trained in the final stage. The argument is that hyperparams often have a consistent effect as the model gets deeper, so it’s okay to tune on a small network and copy the settings to a larger one. It’s not clear whether a similar trick was used in their baselines. If they weren’t, the runtime comparisons arne’t as impressive as they look.

The approach only works if it makes sense for the features to be sparse. This isn’t that big a disadvantage, in my experience neural net hyperparams are nearly independent. This is backed up by the Harmonica results - the important terms are almost all degree 1. (The authors observed that they did need d≥2 for best performance.)

The derivation only works with discrete, binary features. Extending the approach to arbitrary discrete hyperparams doesn’t look too bad (just take the closest power of 2), but extending to continuous spaces looks quite a bit trickier.


