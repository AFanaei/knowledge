# [First Return Then Explore](https://arxiv.org/abs/2004.12919)
tags:
- [[reinforcement-learning]]

refrence:
```
@article{ecoffet2020first,
  title={First return then explore},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:2004.12919},
  year={2020}
}
```

some quote from [alexirpan.com](https://www.alexirpan.com/2020/05/07/rl-potpourri.html) about this paper:

> This is a continuation and extension of the Go-Explore work from UberAI. Both First Return Then Explore and Go-Explore aim to first return to a state that has been visited before, then explore from that state. To make this more efficient, states are grouped into “cells” through some encoding. In the original Go-Explore paper, these cells are defined by downsampling by a fixed factor. First Return Then Explore changes this to tune the downsampling factor online, by doing a small search to maximize normalized entropy across a fixed budget of TT cells. There are also more heuristics on choosing which cell to return to, instead of uniformly at random.

> The part I care about is the part they call Policy-based Go-Explore. My main criticism of the original Go-Explore paper was that it required access to a deterministic analogue of your final environment. They proposed learning a goal-conditioned policy to return to previous states, instead of following a memorized trajectory, which lets you hand stochastic environments at training time. However, they left it as future work.

> the updated paper improved its strengths, but only mildly improved its weaknesses. The paper is an even stronger case that good exploration can be reduced to learning to quickly return to states you’ve visited before, and exploration algorithms without this capability have failure modes that First Return Then Explore fixes. Learning that return policy, however, is still an open problem for general domains. The reduction is valuable, and I hope it encourages more work on efficiently learning goal-conditioned policies.

[//begin]: # "Autogenerated link references for markdown compatibility"
[reinforcement-learning]: ..\reinforcement-learning "Reinforcement Learning"
[//end]: # "Autogenerated link references"