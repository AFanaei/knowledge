# [Data augmentation](https://www.alexirpan.com/2020/05/07/rl-potpourri.html)
tags:
- [[reinforcement-learning]]

refrence: [link](https://arxiv.org/abs/2005.01643)
```
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}
```

> By offline reinforcement learning, they mean reinforcement learning from a fixed dataset of episodes from an environment, without doing any additional online data collection during learning. This is to distinguish it from off-policy learning, which can happen in an offline setting, but is commonly used in settings with frequent online data collection.

> Offline RL is, in my opinion, a criminally understudied subject. It’s both very important and very difficult, and I’ve been talking about writing a blog post about it for over a year. Suffice it to say that I think this tutorial is worth reading. Even if you do not plan to research offline RL, I feel the arguments for why it’s important and why it’s hard are useful to understand, even if you disagree with them.

[//begin]: # "Autogenerated link references for markdown compatibility"
[reinforcement-learning]: ..\reinforcement-learning "Reinforcement Learning"
[//end]: # "Autogenerated link references"