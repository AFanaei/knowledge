# [Wasserstein GAN](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
tags:
- [[reinforcement-learning]]

related:
- https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=A+single+network+adaptive+critic+%28SNAC%29+architecture+for+optimal+control+synthesis+for+a+class+of+nonlinear+systems&btnG=
- https://arxiv.org/abs/1406.2661

refrence: [link](https://arxiv.org/abs/1701.07875)
```
@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}
```

### why is this paper good
Training algorithm is backed up by theory. In deep learning, not all theory-justified papers have good empirical results, but theory-justified papers with good empirical results have really good empirical results. For those papers, it’s very important to understand their theory, because the theory usually explains why they perform so much better.

There is an argument that GANs have close connections to actor-critic reinforcement learning. (See [Pfau & Vinyals 2017](https://arxiv.org/abs/1610.01945).) so we should probably keep an eye on GAN papers anyways.

#### Introduction

When learning generative models, we assume the data we have comes from some unknown distribution P_r. (The r stands for real.) We want to learn a distribution P_θ that approximates P_r, where θ are the parameters of the distribution.

You can imagine two approaches for doing this.

* Directly learn the probability density function P_θ. Meaning, P_θ
​​  is some differentiable function such that P_θ(x)≥0 and ∫P_θ(x)dx=1. We optimize P_θ through maximum likelihood estimation.
* Learn a function that transforms an existing distribution Z into P_θ. Here, g_θ is some differentiable function, Z is a common distribution (usually uniform or Gaussian), and P_θ=g_θ(Z).

MLE is not defined everywhere To deal with this, we can add random noise to P_θ when training the MLE. This ensures the distribution is defined everywhere. But now we introduce some error, and empirically people have needed to add a lot of random noise to make models train. That kind of sucks. Additionally, even if we learn a good density P_​θ, it may be computationally expensive to sample from P_θ.

This motivates the latter approach, of learning a g_​θ (a generator) to transform a known distribution Z. The other motivation is that it’s very easy to generate samples. Given a trained g_θ, simply sample random noise z∼Z, and evaluate g_θ(z). (The downside of this approach is that we don’t explicitly know what P_θ, but in practice this isn’t that important.)

given a distance d, we can treat d(P_r, P_θ) as a loss function. Minimizing d(P_r, P_θ) with respect to θ will bring P_θ close to P_r.

#### diffrent distances:
* The Total Variation (TV) distance
* The Kullback-Leibler (KL) divergence
* The reverse KL divergence
* The Jenson-Shannon (JS) divergence
* the Earth Mover (EM) or Wasserstein distance

#### what is Wasserstein distance
Probability distributions are defined by how much mass they put on each point. Imagine we started with distribution P_r, and wanted to move mass around to change the distribution into P_g. Moving mass mm by distance dd costs m⋅d effort. The earth mover distance is the minimal effort we need to spend.

##### example
Consider probability distributions defined over R^​. Let the true data distribution be (0,y), with y sampled uniformly from U[0,1]. Consider the family of distributions P_θ, where P_θ=(θ,y), with y also sampled from U[0,1].

We’d like our optimization algorithm to learn to move θ to 0, As θ→0, the distance d(P_0, P_θ) should decrease.
* The Total Variation (TV) gives 0 when θ=0 and 1 otherwise.
* The Kullback-Leibler (KL) divergence is +∞ if there is any point (x,y) where P(x,y)>0 and Q(x,y)=0
* The Jenson-Shannon (JS) divergence gives 0 when θ=0 and log(2) otherwise.
* the Earth Mover (EM) works! This gives W(P_0, P_θ)=∣θ∣

This example shows that there exist sequences of distributions that don’t converge under the JS, KL, reverse KL, or TV divergence, but which do converge under the EM distance and shows that for the JS, KL, reverse KL, and TV divergence, there are cases where the gradient is always 0. so any approach that works by taking the gradient ∇_θd(P_0,P_θ) will fail in these cases.

##### two theoriems of the paper
 * out of JS, KL, and Wassertstein distance, only the Wasserstein distance has guarantees of continuity and differentiability, which are both things you really want in a loss function.
 * this proves that every distribution that converges under the KL, reverse-KL, TV, and JS divergences also converges under the Wasserstein divergence. It also proves that a small earth mover distance corresponds to a small difference in distributions.

#### Wasserstein GAN
Unfortunately, computing the Wasserstein distance exactly is intractable.

W(P_​r,P_g)= inf_{γ∈Π(P_r,P_g)} E_{​(x,y)∼γ} [∥x−y∥]

a way to approzimate is to use:

K⋅W(P_r,P_θ)=sup_{∥f∥_L≤K} E_{x∼P_r} [f(x)]− E_{x∼P_θ} [f(x)]

supremum is taken over all K-Lipschitz functions.

##### what is Lipschitz functions:
Let d_X and d_Y be distance functions on spaces X and Y. A function f:X→Y is K-Lipschitz if for all x_1,x_2∈X,

d_Y(f(x_1), f(x_2)) ≤ K d_X(x_1, x_2)

Intuitively, the slope of a K-Lipschitz function never exceeds K, for a more general definition of slope.

Suppose we have a parametrized function family \{f_w\}_{w∈W}, where w are the weights and W is the set of all possible weights. Further suppose these functions are all K-Lipschitz for some K. Then we have

​max_{​w∈W} E_{x∼P_r} [f_w(x)] − E_{x∼P_θ} [f_w(x)] ≤ sup_{∥f∥_L≤K} E_{x∼P_r} [f(x)]− E_{x∼P_θ} [f(x)] ≤ K⋅W(P_r,P_θ)

#### training algorithm
The training process has now broken into three steps.

* For a fixed θ, compute an approximation of W(P_r, P_θ) by training f_w to convergence.
* Once we find the optimal f_w, compute the θ gradient -E_{z∼Z}[∇_θ f_w (g_θ)(z))] by sampling several z∼Z.
* Update \thetaθ, and repeat the process.
There’s one final detail. This entire derivation only works when the function family \{f_w\}is K-Lipschitz. To guarantee this is true, we use weight clamping. The weights w are constrained to lie within [−c,c], by clipping w after every update to w.​​

[The algorithm](https://www.alexirpan.com/public/wasserstein/algorithm.png)

#### Compare & Contrast: Standard GANs
* In GANS, discriminator uses a function D and the output of D should be probability distribution
In WGANs, nothing requires f_w to output a probability.
* The original GAN paper showed that in the limit, the maximum of the discriminator objective above is the Jenson-Shannon divergence, up to scaling and constant factors.
In WGANs, it’s the Wasserstein distance instead.
* in GANs We aren’t updating G against the Jenson-Shannon divergence, or even an approximation of the Jenson-Shannon divergence, we’re updating G against an objective that kind of aims towards the JS divergence, but doesn’t go all the way. It certainly works, but in light of the points this paper makes about gradients of the JS divergence, it’s a bit surprising it does work!
In contrast, because the Wasserstein distance is differentiable nearly everywhere, we can (and should) train f_w to convergence before each generator update, to get as accurate an estimate of W(P_r, P_θ) as possible.

#### questions:
* How important is c (clamping value for weights) for performance?
* Given a fixed critic architecture and fixed cc for clamping, can we quantitatively compare different generators by computing the Wasserstein estimate of both?
* How important is it to train the critic to convergence?
* What ideas from this work are applicable to actor-critic RL?


[//begin]: # "Autogenerated link references for markdown compatibility"
[reinforcement-learning]: ..\reinforcement-learning "Reinforcement Learning"
[//end]: # "Autogenerated link references"