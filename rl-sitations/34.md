# Key concepts in RL
tags:
- [[rl-intro]]

refrence:
```
@misc{rlintro,
    title={Key Concepts in RL},
    author={spinop},
    howpublished={\url{https://spinningup.openai.com/en/latest/spinningup/rl_intro.html}},
    year={2019}
}
```

## general definition
#### The RL Problem
Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T -step trajectory is:

P(\tau|\pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t).

The expected return (for whichever measure), denoted by J(\pi), is then:

J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau) = \underE{\tau\sim \pi}{R(\tau)}.

The central optimization problem in RL can then be expressed by

\pi^* = \arg \max_{\pi} J(\pi),

with \pi^* being the optimal policy.

#### Value Functions
There are four main functions of note here.

1. The On-Policy Value Function, V^{\pi}(s), which gives the expected return if you start in state s and always act according to policy \pi:

V^{\pi}(s) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}

2. The On-Policy Action-Value Function, Q^{\pi}(s,a), which gives the expected return if you start in state s, take an arbitrary action a (which may not have come from the policy), and then forever after act according to policy \pi:

Q^{\pi}(s,a) = \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}

3. The Optimal Value Function, V^*(s), which gives the expected return if you start in state s and always act according to the optimal policy in the environment:

V^*(s) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s\right.}

4. The Optimal Action-Value Function, Q^*(s,a), which gives the expected return if you start in state s, take an arbitrary action a, and then forever after act according to the optimal policy in the environment:

Q^*(s,a) = \max_{\pi} \underE{\tau \sim \pi}{R(\tau)\left| s_0 = s, a_0 = a\right.}

#### Bellman equations
The basic idea behind the Bellman equations is this:

The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.
The Bellman equations for the on-policy value functions are

\begin{align*}
V^{\pi}(s) &= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
Q^{\pi}(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
\end{align*}

where s' \sim P is shorthand for s' \sim P(\cdot |s,a), indicating that the next state s' is sampled from the environment’s transition rules; a \sim \pi is shorthand for a \sim \pi(\cdot|s); and a' \sim \pi is shorthand for a' \sim \pi(\cdot|s').

The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation

## types of RL Algorithm
#### Model-Free vs Model-Based RL
The main upside to having a model is that it allows the agent to plan by thinking ahead

The main downside is that a ground-truth model of the environment is usually not available to the agent. If an agent wants to use a model in this case, it has to learn the model purely from experience, which creates several challenges. The biggest challenge is that bias in the model can be exploited by the agent

## what to learn
##### model free RL
**Policy Optimization**: Methods in this family represent a policy explicitly as \pi_{\theta}(a|s). They optimize the parameters \theta either directly by gradient ascent on the performance objective J(\pi_{\theta}), or indirectly, by maximizing local approximations of J(\pi_{\theta}). This optimization is almost always performed on-policy, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator V_{\phi}(s) for the on-policy value function V^{\pi}(s), which gets used in figuring out how to update the policy.

like: A2C / A3C, PPO,

**Q-Learning**. Methods in this family learn an approximator Q_{\theta}(s,a) for the optimal action-value function, Q^*(s,a). Typically they use an objective function based on the Bellman equation. This optimization is almost always performed off-policy, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. The corresponding policy is obtained via the connection between Q^* and \pi^*: the actions taken by the Q-learning agent are given by

a(s) = \arg \max_a Q_{\theta}(s,a).

like: DQN, C51

**Trade-offs Between Policy Optimization and Q-Learning**. The primary strength of policy optimization methods is that they are principled, in the sense that you directly optimize for the thing you want. This tends to make them stable and reliable. By contrast, Q-learning methods only indirectly optimize for agent performance, by training Q_{\theta} to satisfy a self-consistency equation. There are many failure modes for this kind of learning, so it tends to be less stable. [1] But, Q-learning methods gain the advantage of being substantially more sample efficient when they do work, because they can reuse data more effectively than policy optimization techniques.

**Interpolating Between Policy Optimization and Q-Learning**. Serendipitously, policy optimization and Q-learning are not incompatible (and under some circumstances, it turns out, equivalent), and there exist a range of algorithms that live in between the two extremes. Algorithms that live on this spectrum are able to carefully trade-off between the strengths and weaknesses of either side. Examples include

like DDPG, SAC

**note**:
For more information about how and why Q-learning methods can fail, see 1) this classic paper by Tsitsiklis and van Roy, 2) the (much more recent) review by Szepesvari (in section 4.3.2), and 3) chapter 11 of Sutton and Barto, especially section 11.3 (on “the deadly triad” of function approximation, bootstrapping, and off-policy data, together causing instability in value-learning algorithms).

##### model based RL
**Background: Pure Planning (MPC)**. The most basic approach never explicitly represents the policy, and instead, uses pure planning techniques like model-predictive control (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.

like MBMF

**Expert Iteration**. A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, \pi_{\theta}(a|s). The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an “expert” relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm’s output.

like ExIt algorithm, AlphaZero

## Intro to Policy Optimization
#### Simplest policy Grad
We would like to optimize the policy by gradient ascent, eg

\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}.

This involves two steps:
1. deriving the analytical gradient of policy performance, which turns out to have the form of an expected value, and then 
2. forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.

\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} & \\
&= \nabla_{\theta} \int_{\tau} P(\tau|\theta) R(\tau) & \text{Expand expectation} \\
&= \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau) & \text{Bring gradient under integral} \\
&= \int_{\tau} P(\tau|\theta) \nabla_{\theta} \log P(\tau|\theta) R(\tau) & \text{Log-derivative trick} \\
&= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau|\theta) R(\tau)} & \text{Return to expectation form} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)} & \text{Expression for grad-log-prob}
\end{align*}

If we collect a set of trajectories \mathcal{D} = \{\tau_i\}_{i=1,...,N} where each trajectory is obtained by letting the agent act in the environment using the policy \pi_{\theta},

\hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau),